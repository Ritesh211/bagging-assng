{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ANSWERS-------"
      ],
      "metadata": {
        "id": "lggJbrN8eWF1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1\n",
        "**What is the fundamental idea behind ensemble techniques? How does bagging differ from boosting in terms of approach and objective?**\n",
        "\n",
        "**Answer (summary):**\n",
        "\n",
        "- **Fundamental idea:** Combine multiple models (base learners/weak learners) to form a stronger predictor. Aggregation (voting, averaging, meta-learning) reduces errors that single models make.\n",
        "- **Bagging (Bootstrap Aggregating):**\n",
        "  - Train many models in parallel on bootstrap samples.\n",
        "  - Aggregate predictions by majority vote (classification) or average (regression).\n",
        "  - Objective: reduce variance and overfitting (useful for high-variance models like deep decision trees).\n",
        "- **Boosting:**\n",
        "  - Train models sequentially; each model focuses on mistakes of the previous ones.\n",
        "  - Aggregate with weighted sum/vote; later learners correct earlier errors.\n",
        "  - Objective: reduce bias (turn many weak learners into a strong learner).\n",
        "Markdown cell (Q2):\n",
        "\n",
        "markdown\n",
        "Copy code\n",
        "## Question 2\n",
        "**Explain how the Random Forest Classifier reduces overfitting compared to a single decision tree. Mention the role of two key hyperparameters in this process.**\n",
        "\n",
        "**Answer (summary):**\n",
        "\n",
        "- **Mechanism:** Build an ensemble of decision trees trained on different bootstrap samples and random subsets of features, then average/vote to smooth predictions (reduces variance).\n",
        "- **Key hyperparameters:**\n",
        "  - `n_estimators` (number of trees): more trees -> lower variance (up to diminishing returns).\n",
        "  - `max_features` (features considered per split): smaller value -> more diverse trees -> reduce correlation -> better generalization.\n",
        "- **Other controls:** `max_depth`, `min_samples_leaf`, `min_samples_split` also prevent overly complex trees.\n",
        "Markdown cell (Q3):\n",
        "\n",
        "markdown\n",
        "Copy code\n",
        "## Question 3\n",
        "**What is Stacking in ensemble learning? How does it differ from traditional bagging/boosting methods? Provide a simple example use case.**\n",
        "\n",
        "**Answer (summary):**\n",
        "\n",
        "- **Stacking:** Train multiple base models (level-0) and then train a meta-model (level-1) on base models’ predictions. The meta-model learns how to combine base outputs.\n",
        "- **Difference:** Bagging averages/votes; boosting weights learners sequentially; stacking trains a second-level model to combine predictions (requires out-of-fold predictions to avoid leakage).\n",
        "- **Example:** Base: RandomForest, SVM, LogisticRegression → Meta: LogisticRegression trained on their predicted probabilities to improve final predictions.\n",
        "Markdown cell (Q4):\n",
        "\n",
        "markdown\n",
        "Copy code\n",
        "## Question 4\n",
        "**What is the OOB Score in Random Forest, and why is it useful? How does it help in model evaluation without a separate validation set?**\n",
        "\n",
        "**Answer (summary):**\n",
        "\n",
        "- **OOB (Out-Of-Bag):** For each tree, ~1/3 of training samples are not used (OOB) in its bootstrap sample. Use those to evaluate that tree. Aggregate OOB predictions across trees to estimate generalization performance.\n",
        "- **Why useful:** Provides an internal validation estimate without holding out a separate validation set — efficient use of data and fast approximate CV.\n",
        "Markdown cell (Q5):\n",
        "\n",
        "markdown\n",
        "Copy code\n",
        "## Question 5\n",
        "**Compare AdaBoost and Gradient Boosting in terms of:**\n",
        "- How they handle errors from weak learners\n",
        "- Weight adjustment mechanism\n",
        "- Typical use cases\n",
        "\n",
        "**Answer (summary):**\n",
        "\n",
        "- **AdaBoost:**\n",
        "  - Emphasizes misclassified samples by increasing their sample weights.\n",
        "  - Learners get weights based on performance; next learner trained on re-weighted data.\n",
        "  - Good when using stumps; sensitive to outliers.\n",
        "- **Gradient Boosting:**\n",
        "  - Fits new learners to residuals (negative gradients) of the loss function.\n",
        "  - Uses learning rate (shrinkage) and regularization; more flexible (arbitrary loss).\n",
        "  - Modern variants (XGBoost/LightGBM/CatBoost) excel on tabular data and large datasets.\n",
        "Markdown cell (Q6):\n",
        "\n",
        "markdown\n",
        "Copy code\n",
        "## Question 6\n",
        "**Why does CatBoost perform well on categorical features without requiring extensive preprocessing? Briefly explain its handling of categorical variables.**\n",
        "\n",
        "**Answer (summary):**\n",
        "\n",
        "- CatBoost uses **ordered target statistics** (target encoding without leakage) and permutation-driven encodings to transform categorical features safely.\n",
        "- It also builds combinations of categorical features and uses symmetric tree structures, which together reduce overfitting.\n",
        "- Result: Minimal manual preprocessing required — pass categorical feature indices to CatBoost and it handles encoding internally.\n",
        "Markdown cell (Q7 header and instructions):\n",
        "\n",
        "markdown\n",
        "Copy code\n",
        "## Question 7 — KNN Classifier Assignment: Wine Dataset Analysis with Optimization\n",
        "**Task summary:** Load wine dataset, split (70/30), train KNN (k=5) without scaling and with StandardScaler, evaluate metrics, run GridSearchCV for k=1..20 and metric in {euclidean, manhattan}, compare optimized KNN.\n",
        "\n",
        "Below are code cells to run sequentially.\n",
        "Code cell (Q7 setup + evaluation) — copy into a code cell and run:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "# Q7: KNN on Wine dataset (70/30 split), comparison: unscaled, scaled, and GridSearch optimization\n",
        "# Run this cell in Colab.\n",
        "\n",
        "# Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load data\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "target_names = data.target_names\n",
        "\n",
        "# 70/30 split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.30, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# 1) KNN without scaling (k=5)\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "y_pred = knn.predict(X_test)\n",
        "acc_unscaled = accuracy_score(y_test, y_pred)\n",
        "print(\"KNN WITHOUT scaling — Accuracy: {:.4f}\".format(acc_unscaled))\n",
        "print(\"Classification report (unscaled):\\n\", classification_report(y_test, y_pred, target_names=target_names))\n",
        "\n",
        "# 2) Scale (StandardScaler) and retrain\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = knn_scaled.predict(X_test_scaled)\n",
        "acc_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "print(\"\\nKNN WITH StandardScaler — Accuracy: {:.4f}\".format(acc_scaled))\n",
        "print(\"Classification report (scaled):\\n\", classification_report(y_test, y_pred_scaled, target_names=target_names))\n",
        "\n",
        "# 3) GridSearchCV to find best K and metric\n",
        "param_grid = {\n",
        "    'n_neighbors': list(range(1, 21)),\n",
        "    'metric': ['euclidean', 'manhattan']\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid.fit(X_train_scaled, y_train)  # use scaled features for grid search\n",
        "print(\"\\nGridSearchCV best params:\", grid.best_params_)\n",
        "print(\"GridSearchCV best CV score: {:.4f}\".format(grid.best_score_))\n",
        "\n",
        "# 4) Train optimized KNN on scaled data and compare\n",
        "best_knn = grid.best_estimator_\n",
        "best_knn.fit(X_train_scaled, y_train)\n",
        "y_pred_best = best_knn.predict(X_test_scaled)\n",
        "acc_best = accuracy_score(y_test, y_pred_best)\n",
        "print(\"\\nOptimized KNN — Test Accuracy: {:.4f}\".format(acc_best))\n",
        "print(\"Classification report (optimized):\\n\", classification_report(y_test, y_pred_best, target_names=target_names))\n",
        "\n",
        "# Summary\n",
        "print(\"Summary of Accuracies:\\n Unscaled: {:.4f}\\n Scaled: {:.4f}\\n Optimized: {:.4f}\".format(acc_unscaled, acc_scaled, acc_best))\n",
        "Markdown cell (Q8 header and instructions):\n",
        "\n",
        "markdown\n",
        "Copy code\n",
        "## Question 8 — PCA + KNN with Variance Analysis and Visualization\n",
        "**Task summary:** Load Breast Cancer dataset, apply StandardScaler, make scree plot (explained variance ratio), keep 95% variance, train KNN on original and PCA data, compare accuracies, and scatter plot first two PCs colored by class.\n",
        "Code cell (Q8 implementation + plots) — copy into a code cell and run:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "# Q8: PCA + KNN on Breast Cancer dataset, scree plot, retain 95% variance, compare accuracy, scatter plot of first 2 PCs.\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "target_names = data.target_names\n",
        "\n",
        "# Standardize\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# PCA full for scree plot\n",
        "pca_full = PCA()\n",
        "X_pca_full = pca_full.fit_transform(X_scaled)\n",
        "explained_ratio = pca_full.explained_variance_ratio_\n",
        "\n",
        "# Scree plot\n",
        "plt.figure(figsize=(9,5))\n",
        "plt.plot(np.arange(1, len(explained_ratio) + 1), explained_ratio, marker='o')\n",
        "plt.title(\"Scree Plot: Explained Variance Ratio\")\n",
        "plt.xlabel(\"Principal Component\")\n",
        "plt.ylabel(\"Explained Variance Ratio\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Retain 95% variance\n",
        "pca95 = PCA(n_components=0.95)\n",
        "X_pca95 = pca95.fit_transform(X_scaled)\n",
        "print(\"Original features:\", X.shape[1])\n",
        "print(\"Reduced features (retain 95% variance):\", X_pca95.shape[1])\n",
        "\n",
        "# Train-test split for original scaled data and PCA data (use same random_state for comparability)\n",
        "X_train_orig, X_test_orig, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42, stratify=y)\n",
        "X_train_pca, X_test_pca, _, _ = train_test_split(X_pca95, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# KNN on original scaled data\n",
        "knn_orig = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_orig.fit(X_train_orig, y_train)\n",
        "acc_orig = accuracy_score(y_test, knn_orig.predict(X_test_orig))\n",
        "print(\"\\nKNN accuracy on original scaled data: {:.4f}\".format(acc_orig))\n",
        "\n",
        "# KNN on PCA reduced data\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_pca.fit(X_train_pca, y_train)\n",
        "acc_pca = accuracy_score(y_test, knn_pca.predict(X_test_pca))\n",
        "print(\"KNN accuracy on PCA-reduced (95% var) data: {:.4f}\".format(acc_pca))\n",
        "\n",
        "# Visualization: first two principal components (full PCA transform)\n",
        "plt.figure(figsize=(8,6))\n",
        "scatter = plt.scatter(X_pca_full[:,0], X_pca_full[:,1], c=y, cmap='coolwarm', s=50, alpha=0.7, edgecolor='k')\n",
        "plt.title(\"Breast Cancer — First Two Principal Components\")\n",
        "plt.xlabel(\"PC1\")\n",
        "plt.ylabel(\"PC2\")\n",
        "plt.legend(handles=scatter.legend_elements()[0], labels=list(target_names))\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "Markdown cell (Q9 header and instructions):\n",
        "\n",
        "markdown\n",
        "Copy code\n",
        "## Question 9 — KNN Regressor with Distance Metrics and K-Value Analysis\n",
        "**Task summary:** Create synthetic regression data (500 samples, 10 features), compare KNN regressor with Euclidean and Manhattan (K=5), compute MSEs, test K={1,5,10,20,50} and plot K vs MSE.\n",
        "Code cell (Q9 implementation + plot) — copy into a code cell and run:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "# Q9: KNN Regressor — distance metrics and K analysis (MSE vs K)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate synthetic regression data\n",
        "X, y = make_regression(n_samples=500, n_features=10, noise=15.0, random_state=42)\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_s = scaler.fit_transform(X_train)\n",
        "X_test_s = scaler.transform(X_test)\n",
        "\n",
        "# K=5 Euclidean\n",
        "knn_euc = KNeighborsRegressor(n_neighbors=5, metric='euclidean')\n",
        "knn_euc.fit(X_train_s, y_train)\n",
        "mse_euc = mean_squared_error(y_test, knn_euc.predict(X_test_s))\n",
        "\n",
        "# K=5 Manhattan\n",
        "knn_man = KNeighborsRegressor(n_neighbors=5, metric='manhattan')\n",
        "knn_man.fit(X_train_s, y_train)\n",
        "mse_man = mean_squared_error(y_test, knn_man.predict(X_test_s))\n",
        "\n",
        "print(\"MSE Euclidean (K=5): {:.4f}\".format(mse_euc))\n",
        "print(\"MSE Manhattan (K=5): {:.4f}\".format(mse_man))\n",
        "\n",
        "# K vs MSE analysis\n",
        "K_values = [1, 5, 10, 20, 50]\n",
        "MSE_scores = []\n",
        "for k in K_values:\n",
        "    model = KNeighborsRegressor(n_neighbors=k)\n",
        "    model.fit(X_train_s, y_train)\n",
        "    y_pred = model.predict(X_test_s)\n",
        "    MSE_scores.append(mean_squared_error(y_test, y_pred))\n",
        "\n",
        "# Plot K vs MSE\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(K_values, MSE_scores, marker='o')\n",
        "plt.title(\"K vs MSE (KNN Regressor)\")\n",
        "plt.xlabel(\"K\")\n",
        "plt.ylabel(\"Mean Squared Error\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Print table\n",
        "for k, mse in zip(K_values, MSE_scores):\n",
        "    print(\"K = {:>2} -> MSE = {:.4f}\".format(k, mse))\n",
        "Markdown cell (Q10 header and instructions):\n",
        "\n",
        "markdown\n",
        "Copy code\n",
        "## Question 10 — KNN with KD-Tree/Ball Tree, Imputation, and Real-World Data (Pima Indians Diabetes)\n",
        "**Task summary:** Load Pima Indians Diabetes data (contains missing values), perform KNNImputer, scale features, train KNN with three algorithms (brute, kd_tree, ball_tree), compare training/prediction time and accuracy, and plot decision boundary using two most important features.\n",
        "\n",
        "**Note:** This cell downloads the dataset from an online source (UCI / Kaggle-like raw CSV link). If the link is blocked, upload `pima-indians-diabetes.csv` manually into Colab.\n",
        "Code cell (Q10 implementation + download helper + decision boundary) — copy into a code cell and run:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "# Q10: KNN Imputer + compare brute/kd_tree/ball_tree on Pima Indians Diabetes dataset\n",
        "# This code attempts to download the dataset. If it fails, upload the CSV to Colab manually.\n",
        "\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Try to download the dataset (common raw CSV on GitHub). If internet blocked, upload file manually.\n",
        "csv_path = 'pima-indians-diabetes.csv'\n",
        "if not os.path.exists(csv_path):\n",
        "    try:\n",
        "        url = \"https://raw.githubusercontent.com/selva86/datasets/master/PimaIndiansDiabetes.csv\"\n",
        "        df = pd.read_csv(url)\n",
        "        df.to_csv(csv_path, index=False)\n",
        "        print(\"Downloaded dataset from:\", url)\n",
        "    except Exception as e:\n",
        "        print(\"Automatic download failed. Please upload 'pima-indians-diabetes.csv' to Colab. Error:\", e)\n",
        "        raise SystemExit(\"Upload dataset and re-run.\")\n",
        "\n",
        "# Read dataset\n",
        "df = pd.read_csv(csv_path)\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "print(df.head())\n",
        "\n",
        "# Dataset may already have column 'Outcome' or 'diabetes'; normalize column names\n",
        "# Known column names from Selva86 dataset: 'pregnant','glucose','pressure','triceps','insulin','mass','pedigree','age','diabetes'\n",
        "# Ensure final columns: features + 'Outcome' (0/1)\n",
        "if 'Outcome' not in df.columns:\n",
        "    # attempt to standardize\n",
        "    if 'diabetes' in df.columns:\n",
        "        df = df.rename(columns={'diabetes':'Outcome'})\n",
        "    elif 'Diabetes' in df.columns:\n",
        "        df = df.rename(columns={'Diabetes':'Outcome'})\n",
        "    else:\n",
        "        # try last column as outcome\n",
        "        df.columns = list(df.columns[:-1]) + ['Outcome']\n",
        "\n",
        "# Replace zeros with NaN for some columns (common practice)\n",
        "cols_with_zero = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI',\n",
        "                  'glucose','pressure','triceps','insulin','mass']  # include alternative names\n",
        "for col in df.columns:\n",
        "    if col in cols_with_zero:\n",
        "        df[col] = df[col].replace(0, np.nan)\n",
        "\n",
        "# If there are mixed column names, ensure numeric matrix and an 'Outcome' column\n",
        "# Show missing counts\n",
        "print(\"\\nMissing value counts:\\n\", df.isna().sum())\n",
        "\n",
        "# Prepare X, y\n",
        "# Map columns so that drop 'Outcome' column for features\n",
        "if 'Outcome' not in df.columns:\n",
        "    raise ValueError(\"Outcome column not found; please ensure dataset contains outcome column named 'Outcome' or 'diabetes'.\")\n",
        "\n",
        "# Use all non-Outcome columns as features\n",
        "X_raw = df.drop('Outcome', axis=1)\n",
        "y = df['Outcome'].astype(int).values\n",
        "\n",
        "# KNN Imputer\n",
        "imputer = KNNImputer(n_neighbors=5)\n",
        "X_imputed = imputer.fit_transform(X_raw)\n",
        "\n",
        "# Scale\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_imputed)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# Compare algorithms\n",
        "methods = ['brute', 'kd_tree', 'ball_tree']\n",
        "results = {}\n",
        "for method in methods:\n",
        "    model = KNeighborsClassifier(n_neighbors=5, algorithm=method)\n",
        "    t0 = time.time()\n",
        "    model.fit(X_train, y_train)\n",
        "    t1 = time.time()\n",
        "    preds = model.predict(X_test)\n",
        "    t2 = time.time()\n",
        "    results[method] = {\n",
        "        'train_time_sec': t1 - t0,\n",
        "        'predict_time_sec': t2 - t1,\n",
        "        'accuracy': accuracy_score(y_test, preds),\n",
        "        'confusion_matrix': confusion_matrix(y_test, preds)\n",
        "    }\n",
        "\n",
        "# Print results\n",
        "for k, v in results.items():\n",
        "    print(f\"\\nMethod: {k}\")\n",
        "    print(\" Train time (s):\", v['train_time_sec'])\n",
        "    print(\" Predict time (s):\", v['predict_time_sec'])\n",
        "    print(\" Accuracy:\", v['accuracy'])\n",
        "    print(\" Confusion matrix:\\n\", v['confusion_matrix'])\n",
        "\n",
        "# Select best method by accuracy (tie-breaker: fastest predict time)\n",
        "best_method = max(results.items(), key=lambda x: (x[1]['accuracy'], -x[1]['predict_time_sec']))[0]\n",
        "print(\"\\nBest method selected:\", best_method)\n",
        "\n",
        "# Decision boundary visualization using two most important features (from DecisionTree)\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "importances = dt.feature_importances_\n",
        "top2 = np.argsort(importances)[-2:]\n",
        "print(\"Top 2 feature indices for visualization:\", top2)\n",
        "\n",
        "# Prepare 2D train/test using top2 features\n",
        "X2_train = X_train[:, top2]\n",
        "X2_test = X_test[:, top2]\n",
        "\n",
        "# Retrain best KNN on 2D features (for visualization)\n",
        "knn_best2 = KNeighborsClassifier(n_neighbors=5, algorithm=best_method)\n",
        "knn_best2.fit(X2_train, y_train)\n",
        "\n",
        "# Create meshgrid\n",
        "x_min, x_max = X2_train[:, 0].min() - 1, X2_train[:, 0].max() + 1\n",
        "y_min, y_max = X2_train[:, 1].min() - 1, X2_train[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300), np.linspace(y_min, y_max, 300))\n",
        "grid = np.c_[xx.ravel(), yy.ravel()]\n",
        "Z = knn_best2.predict(grid).reshape(xx.shape)\n",
        "\n",
        "# Plot decision boundary\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')\n",
        "plt.scatter(X2_test[:, 0], X2_test[:, 1], c=y_test, cmap='coolwarm', edgecolor='k', s=50)\n",
        "plt.title(f\"KNN decision boundary (best: {best_method}) — using top 2 features\")\n",
        "plt.xlabel(f\"Feature index {top2[0]}\")\n",
        "plt.ylabel(f\"Feature index {top2[1]}\")\n",
        "plt.show()\n",
        "\n",
        "# End of Q10"
      ],
      "metadata": {
        "id": "ax24c5naebAa"
      }
    }
  ]
}